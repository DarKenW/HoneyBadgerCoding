[Rrgularization](https://www.youtube.com/watch?v=5asL5Eq2x0A&list=RD5asL5Eq2x0A&start_radio=1)
 
lets start from bias-variance trade off
fit different model to the data

- simple linear model : y=ax+b   underfitting  High-bias :very different from expected value and the correct value 
- quatradic : y = ax+bx^2+c         its ok 
- n^th polynomial model : y = ax+bx^2+cx^3+dx^4+ex^5+fx^6 + .... zx^27      outfitting    High-variance: sensitive to the data set, once data set changes, model changes a lot as well

So to get okay model, what we usually do: add more terms to simple linear model or remove/reduce magnitude items from n^th polynomial? we usually try to remove items. 
How to do that? Ans: Regularization: add penalty tems to the n^th polynomial model

J(theta) = min[(y-y_bar)^2 + sum(theta^p))]       
R(w) = lambda * (|theta1|^p1 + |theta1|^p2 + |theta1|^p3 + ... )  lambda: regularization term        if lambda is vary large, theta_s turns to be zero, only theta 0

but today, we are not gonna just add R(w)
we see this as a optimization problem: J(theta) = min[(y-y_bar)^2    s.t R(w)<c^2, there always is a c
beta1, beta2 two dimensional graph

J(theta) = F(theta0, theta1, theta2 ... lambda) = min[(y-y_bar)^2 + lambda * (sum(theta^p)-c^2)    take derivative try lambda, find best combination of thetas




